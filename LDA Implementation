_author_ = 'anujsharma'

# Importing all necessary packages to run the code
import math
import re
from nltk.corpus import stopwords
import csv
from gensim import corpora, models, similarities
from gensim.models import hdpmodel, ldamodel, LdaModel
from nltk.corpus import stopwords

# Initializing the variables
mylist=[]
count = 0

# Creating the csv file to collect the output
fileWriter = csv.writer(open("Topic_words.csv", "wb"),delimiter=",")
fileWriter1 = csv.writer(open("Topic_score.csv", "wb"),delimiter=",")

# Main code body starts from here
# Data cleaning - Iteration one
with open('BrazilTweets.csv','rb') as f:
    reader = csv.reader(f)
    for row in reader:
                content = row[0]
                content = content.lower()
                words = re.findall(r'\w+', content,flags = re.UNICODE | re.LOCALE)
#This is the simple way to remove stop words
                important_words=[]
                for word in words:
                     if word not in stopwords.words('portuguese'):
                            important_words.append(word)
                important_string = str(important_words)
                important_string = important_string.replace("'", "")
                important_string = re.sub(" \d+", "", important_string)
                important_string = important_string.replace(",", "")
                important_string = important_string.replace("[", "")
                important_string = important_string.replace("]", "")
                mylist.append(important_string)
                count = count + 1
                if count > 10:
                     break

documents = mylist

# Data cleaning - Iteration 2
texts = [[word for word in document.lower().split() if word not in stopwords.words('portuguese')]
         for document in documents]

all_tokens = sum(texts, [])
tokens_once = set(word for word in set(all_tokens) if all_tokens.count(word) == 1)
texts = [[word for word in text if word not in tokens_once]
         for text in texts]

dictionary = corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]

# LDA Model
lda = LdaModel(corpus, id2word=dictionary, num_topics=10)

#print lda.print_topics(num_words=100,num_topics=10)
for i in range(0, lda.num_topics):
   # print lda.print_topic(i)
    string_topic = lda.print_topic(i)
    topic_words = re.findall(r'\b[a-z]+\b',string_topic)
    word_score = re.findall(r'0.\d+',string_topic)
    tp_number = i + 1
    final_words = []
    final_word_score = []
    for i in topic_words:
        final_words.append(i.encode('UTF8'))
        fileWriter.writerow([i])                              # Writing all the topics to the csv file
    print final_words                                         # Printing the words of all 10 topics

    for i in word_score:
        final_word_score.append(i.encode('UTF8'))
        fileWriter1.writerow([i])                             # Writing all the topic scored to the csv file
    print final_word_score                                    # Printing the score for every word in all 10 topics

# End of code
